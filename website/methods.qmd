---
title: Methods
---

# Transformer: 

Transformer models, first introduced in 2017 by Vaswani et al, take a different approach to sequence modeling by replacing recurrence with self-attention mechanisms. Transformers excel in handling input sequence lengths of different sizes. They are also capable of parallelizing computation across multiple positions, and can extract rich contextual information without running into the vanishing-gradient issues that are common to RNNs. Transformers are notoriously difficult to train, so we knew that having an environment that could facilitate high-speed training was important. With this in mind, we decided to host our transformer on a LambdaLabs instance with a Nvidia A10 GPU to increase training efficiency and allow for quicker model iterations. 

For the goal of detecting collisions through dashcam footage, we built out a classification pipeline using a transformer paired with a BCE loss function to capture the temporal components of our video frame inputs. The process begins by projecting each frameâ€™s raw feature vector into a fixed 128-dimensional embedding space via a linear layer. After this, we apply sinusoidal positional encodings into our feature vector to try and bring back the sequential structure that underlies our raw video data. A LayerNorm preprocessor then stabilizes the embeddings before they are fed into a two layer encoder layer - where each layer uses multi headed attention, and a 512-dimensional feed-forward network using a dropout rate of 0.1 to prevent overfitting. After the encoder, we apply another LayerNorm to prepare our sequence for global aggregation. In this step, we add mask padded positions, sum the amount of valid token embeddings, and divide the sequence lengths to create a single 128-dimensional summary vector per video. For the final step, this vector gets fed into a MLP. For this architecture, we prepare two hidden layers [256, 64], each paired with BatchNorm, ReLU activation, and 0.1 dropout. The output layer consists of two nodes, using sigmoidal activation to yield our final binary classification vector. 
